{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data: (n, 2), n = row of train.csv\n",
    "train_data = pd.read_csv(\"../input/train.csv\", dtype={\n",
    "    \"acoustic_data\": np.float32, \"time_to_failure\": np.float32\n",
    "}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start time for each earthquake, that is, the external last time for each previous earthquake (except the last one)\n",
    "start_indexs = np.nonzero(np.diff(train_data[:, 1]) > 0)[0] + 1\n",
    "start_indexs = np.insert(start_indexs, 0, 0)\n",
    "\n",
    "# normalize each earthquake\n",
    "for i in range(len(start_indexs) - 1):\n",
    "    temp_data = train_data[start_indexs[i]:start_indexs[i+1], 0]\n",
    "    temp_data = (temp_data - temp_data.mean()) / temp_data.std()\n",
    "    train_data[start_indexs[i]:start_indexs[i+1], 0] = temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return: (n_steps, n), n = row of features\n",
    "def features_12(data, n_steps=150, step_length=1000):\n",
    "    data_10  = data[:, -(step_length // 10):]\n",
    "    data_100 = data[:, -(step_length // 100):]\n",
    "    \n",
    "    return np.c_[\n",
    "        data.mean(axis=1),\n",
    "        data.std(axis=1),\n",
    "        data.min(axis=1),\n",
    "        data.max(axis=1),\n",
    "        \n",
    "        data_10.mean(axis=1),\n",
    "        data_10.std(axis=1),\n",
    "        data_10.min(axis=1),\n",
    "        data_10.max(axis=1),\n",
    "        \n",
    "        data_100.mean(axis=1),\n",
    "        data_100.std(axis=1),\n",
    "        data_100.min(axis=1),\n",
    "        data_100.max(axis=1),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data: (n, 1), n = n_steps * step_length\n",
    "def to_features(raw_data, last_index=None, n_steps=150, step_length=1000):\n",
    "    if last_index == None:\n",
    "        last_index = len(raw_data)\n",
    "        \n",
    "    data = raw_data[(last_index - n_steps * step_length):last_index]\n",
    "    data = data.reshape(n_steps, step_length)\n",
    "    \n",
    "    return features_12(data, n_steps, step_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data: (n, 2), n = max_index - min_index\n",
    "def random_generator(raw_data, min_index=0, max_index=None, batch_size=32, n_steps=150, step_length=1000):\n",
    "    if max_index == None:\n",
    "        max_index = len(raw_data)\n",
    "        \n",
    "    while True:\n",
    "        last_indexs = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "                \n",
    "        samples = np.zeros((batch_size, n_steps, step_length))\n",
    "        targets = np.zeros(batch_size)\n",
    "        \n",
    "        for i, last_index in enumerate(last_indexs):\n",
    "            samples[i] = to_features(raw_data[:, 0], last_index, n_steps, step_length)\n",
    "            targets[i] = raw_data[last_index - 1, 1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data: (n, 2), n = max_index - min_index\n",
    "class RandomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, raw_data, min_index=0, max_index=None, n_steps=150, step_length=1000, transform=None, dataset_size=65536):\n",
    "        if max_index == None:\n",
    "            max_index = len(raw_data)\n",
    "        if transform == None:\n",
    "            transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor()\n",
    "            ])\n",
    "        \n",
    "        self.data = raw_data\n",
    "        self.n_steps = n_steps\n",
    "        self.step_length = step_length\n",
    "        self.transform = transform\n",
    "        self.last_indexs = np.random.randint(min_index + n_steps * step_length, max_index, size=dataset_size)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sample = to_features(self.data[:, 0], self.last_indexs[index], self.n_steps, self.step_length)\n",
    "        target = self.data[self.last_indexs[index] - 1, 1]\n",
    "        return self.transform(sample), target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.last_indexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 32\n",
    "n_epochs = 30\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RandomDataset(train_data, dataset_size=65536)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "test_dataset = RandomDataset(train_data, min_index=0, max_index=start_indexs[1], dataset_size=4096)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=48):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size)\n",
    "        self.final_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(10, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output, _ = self.rnn(input)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.final_layers(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=48):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = torch.nn.LSTM(input_size, hidden_size)\n",
    "        self.final_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(10, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output, _ = self.rnn(input)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.final_layers(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=48):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = torch.nn.GRU(input_size, hidden_size)\n",
    "        self.final_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(10, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output, _ = self.rnn(input)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.final_layers(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "model = GRU(12)  # row of features\n",
    "model.to(device)\n",
    "\n",
    "lose_fn = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i, (sample, target) in enumerate(data_loader):\n",
    "        sample, target = sample.to(device), target.to(device)\n",
    "        optimizier.zero_grad()\n",
    "        output = model(sample)\n",
    "        loss = loss_fn(output, target)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, loss_fn, device):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        total_loss = 0\n",
    "        for i, (sample, target) in enumerate(data_loader):\n",
    "            sample, target = sample.to(device), target.to(device)\n",
    "            output = model(sample)\n",
    "            loss = loss_fn(output, target)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_loader, test_loader, model, loss_fn, optimizer, n_epochs, device):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(train_loader, model, loss_fn, optimizer, device)\n",
    "        test_loss = evaluate(test_loader, model, loss_fn, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        show_curve(train_losses, \"train_losses\")\n",
    "        show_curve(test_losses, \"test_losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_curve(ys, title):\n",
    "    x = np.array(range(len(ys)))\n",
    "    y = np.array(ys)\n",
    "    plt.plot(x, y, c='b')\n",
    "    plt.axis()\n",
    "    plt.title('{} Curve:'.format(title))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('{} Value'.format(title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(train_loader, test_loader, model, loss_fn, optimizer, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(model):\n",
    "    submission = pd.read_csv(\"../input/sample_submission.csv\", index_col=\"seg_id\", dtype={ \"time_to_failure\": np.float32 })   \n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    for i, seg_id in enumerate(submission.index):\n",
    "        seg = pd.read_csv(\"../input/test/\" + seg_id + \".csv\", dtype={ \"acoustic_data\": np.float32 }).values\n",
    "        seg = seg[:, 0]\n",
    "        \n",
    "        # normalize\n",
    "        seg = (seg - seg.mean()) / seg.std()\n",
    "        \n",
    "        seg = to_features(seg)\n",
    "        seg = transform(seg)\n",
    "        seg = seg.to(device)\n",
    "        output = model(seg)\n",
    "        submission['time_to_failure'][i] = output\n",
    "    \n",
    "    submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
